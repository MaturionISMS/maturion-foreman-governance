# Bootstrap Execution Learnings

This document records structural learnings identified during
manual bootstrap execution (Wave 0).

These learnings inform future governance canon,
FM application automation, and delegated execution models.

Entries are additive and non-retroactive.

## BL-0001 ‚Äî Governance Stabilisation Must Precede FM Recruitment

**Context:** Pre-Batch 3 (Governance Hardening & Readiness)

**Observed Issue:**  
FM recruitment was initially attempted while governance artifacts and authority boundaries were still in flux.

**Root Cause:**  
No explicit rule existed requiring governance lock and layer-down completion before FM activation.

**Learning:**  
Governance MUST be stabilised, locked, and layered down before FM is recruited or granted planning authority.

**Governance Impact:**  
Informs FM recruitment sequencing and Batch entry conditions.

**Status:** Recorded

## BL-0002 ‚Äî Readiness Certification Is a Governance Function, Not an Execution Trigger

**Context:** Batch 3A ‚Äî Final Readiness Certification

**Observed Issue:**  
Risk of interpreting readiness certification as implicit execution authorization.

**Root Cause:**  
Readiness and execution were not explicitly separated in early mental models.

**Learning:**  
Readiness certification records system state and constraints only; execution authority must be granted explicitly and separately.

**Governance Impact:**  
Clarifies Batch 3A vs Batch 3B boundary.

**Status:** Recorded

## BL-0003 ‚Äî FM Identity Must Be Canonical Before Recruitment

**Context:** Batch 3B Entry Preparation

**Observed Issue:**  
Multiple legacy FM agent definitions created ambiguity in authority and scope.

**Root Cause:**  
Historical agent contracts were not deprecated before reuse.

**Learning:**  
Exactly one canonical FM agent definition MUST exist before FM recruitment. Legacy definitions must be removed or explicitly deprecated.

**Governance Impact:**  
Informs agent canonicalisation requirements and FM onboarding rules.

**Status:** Recorded

## BL-0004 ‚Äî Bootstrap Execution Proxy Is a Governance-Safe Deviation

**Context:** Batch 3B ‚Äî Wave 0 Bootstrap Execution

**Observed Issue:**  
FM could not perform GitHub platform actions prior to full automation.

**Root Cause:**  
Delegated execution pathways were not yet operational.

**Learning:**  
A human execution proxy may perform mechanical platform actions during bootstrap, provided authority, instruction, and auditability remain with FM and governance.

**Governance Impact:**  
Informs Bootstrap Execution Proxy clause and future automated delegation design.

**Status:** Recorded

### BL-0005 ‚Äî Execution Visibility Gap Without Runtime

Context:
During Wave 0.2 task assignment dry run, FM correctly assigned tasks and established execution tracking.
However, CS2 experienced loss of real-time visibility into execution progress.

Observation:
GitHub provides no native mechanism for:
- long-running agent execution
- progress signaling
- background task monitoring
- agent wake/sleep awareness

Impact:
- FM appears inactive after assignment
- CS2 must manually poll for progress
- Execution continuity relies on human vigilance

Conclusion:
This is not an FM or governance defect.
This is an architectural gap requiring a runtime execution monitor.

Resolution Path (Future):
- Implement Maturion Runtime Execution Monitor inside FM App
- Provide UI-level execution state, timers, and alerts
- Enable FM to be event-driven rather than manually polled

Bootstrap Handling:
- During Wave 0 execution, CS2 acts as runtime observer
- Static execution trackers are acceptable temporarily

Formalisation Requirement:
This learning mandates the creation of a canonical specification for a
‚ÄúMaturion Runtime Execution Monitor‚Äù.

This specification MUST be authored before any attempt to automate
execution, delegation, or progress monitoring, and MUST define:
- Responsibilities
- Authority boundaries
- Prohibited behaviors
- Governance integration
- Audit requirements

Reference Specification:
MATURION_RUNTIME_EXECUTION_MONITOR_SPEC.md

### BL-0006 ‚Äî Builder Execution Requires Explicit Simulation During Bootstrap

**Context**

During Wave 0.2 (Controlled Task Assignment Dry Run), tasks were formally assigned by FM
to conceptual builder roles (e.g. `ui-builder`) using governed task assignment documents.

Despite correct planning, validation, tracking, and heartbeat protocols, no task execution
occurred. Tasks remained in `ASSIGNED` state indefinitely.

**Observation**

In the absence of a runtime execution layer:

- Builder roles are declarative, not active
- No mechanism exists to start, run, or complete work
- FM can plan and validate, but cannot trigger execution
- GitHub provides no native long-running agent execution

**Learning**

During bootstrap phases (Wave 0 / Wave 0.x), **builder execution must be explicitly
simulated or proxied**, with clear authorization and auditability.

‚ÄúAssignment‚Äù alone is insufficient ‚Äî execution must be declared.

**Governance Position**

Simulated execution is **not a governance breach** when:

- Explicitly authorized by CS2
- Clearly annotated as SIMULATED
- Limited to documentation-only or non-production artifacts
- Fully auditable via DAI and execution tracker

**Resolution Pattern (Bootstrap Only)**

1. FM assigns task as normal
2. If no runtime execution occurs within bounded time:
   - CS2 authorizes simulated execution
3. FM produces deliverable content via proxy
4. FM validates acceptance criteria
5. FM generates DAI
6. CS2 performs GitHub platform actions as execution proxy
7. Tracker marks task as COMPLETED (SIMULATED)

**Future Resolution (Post-Bootstrap)**

This learning directly motivates creation of:

- `MATURION_RUNTIME_EXECUTION_MONITOR`
- Active task state transitions (ASSIGNED ‚Üí IN_PROGRESS ‚Üí COMPLETED)
- Builder wake/sleep signaling
- UI-level execution visibility

**Status:** Recorded  
**Applicability:** Wave 0 / Bootstrap phases only

### BL-0007 ‚Äî Irresponsible Appointment of Officials Will Collapse the Model (Critical)

**Context**
During transition from bootstrap (Wave 0.x) to production planning (Wave 1.0), the build process drifted toward coder-native execution patterns (implementation-first) and away from the Maturion governed pipeline (True North ‚Üí QA-to-Red ‚Üí Builders Build-to-Green).

**Observation**
This drift occurs when appointed officials (FM / Governance Liaison / Builders) do not internalize the Maturion-first constitution:
- CS2 verifies only UI outcomes
- True North architecture is mandatory
- QA-to-Red defines build tasks
- Builders build to green only
- Governance is constitutional, not advisory

**Root Cause**
Appointment was not treated as a controlled, gated act with explicit constitutional onboarding.
Agent contracts lacked a shared, repo-level ‚ÄúAgent Constitution‚Äù applying uniformly to all officials.

**Learning**
Appointment discipline is a security control.
Incorrect appointment (or incorrect agent mindset) is a platform risk that can negate the entire governance model.

**Requirement**
All officials MUST be appointed using a governed protocol that:
- binds them to BUILD_PHILOSOPHY and canonical governance
- explicitly encodes CS2‚Äôs UI-only verification constraint
- enforces sequencing: True North ‚Üí QA-to-Red ‚Üí Build-to-Green only
- defines escalation triggers and STOP conditions
- prevents coder-first defaults from reappearing under pressure

**Status:** Recorded (Critical)  
**Applies To:** FM, Governance Liaison, Builders, Watchdog roles

### BL-0008 ‚Äî PR Gate Layer-Down Is a Mandatory Prerequisite to Builder Appointment

**Context**
After bootstrap validation (Wave 0.x), FM must formally appoint builders and begin real execution.
This requires enforceable PR merge discipline at the application repository level.

**Observation**
Although PR gate requirements were fully defined at the governance layer, they were not yet
mechanically layered down into the FM application repository as active, role-aware merge controls.

As a result:
- Builders could theoretically be appointed without enforceable merge authority boundaries
- Architecture could be produced without guaranteed build-to-green discipline
- Governance intent existed, but enforcement was incomplete

**Learning**
Builder appointment MUST NOT occur unless PR gate rules are:
- Present in the application repository
- Role-aware (Builder vs FM vs Governance)
- Actively enforceable
- Aligned with canonical governance definitions

Gate layer-down is not optional or implicit.
It is a hard prerequisite to builder appointment and architecture freeze.

**Required Correction**
Before FM appoints builders:
1. PR gate definitions must be layered down from governance
2. Gate ownership and red declarant authority must be enforceable
3. Merge rules must be verifiable in the application repository

**Status:** Recorded  
**Applies To:** All application repositories (FM app, SlotMaster, future apps)

---

## BL-009 ‚Äî Platform Readiness Was Declared Without a Canonical Definition

### Classification
- **Type:** Governance Learning
- **Phase:** Bootstrap (Batch 1‚Äì3)
- **Severity:** Structural
- **Status:** Closed (Learning Captured)
- **Impacts:** All future build readiness declarations

---

### Summary

During the bootstrap execution batches, the platform was declared **‚Äú100% ready for build execution‚Äù** based on informal and incomplete readiness criteria.

Subsequent execution attempts demonstrated that this declaration was **substantively incorrect**, as multiple governance, sequencing, and authority misalignments were discovered that prevented safe, governed execution.

This was not an execution failure, but a **governance definition failure**.

---

### What Was Believed

At the time of declaration, ‚Äúplatform readiness‚Äù was assumed to mean:

- Core repositories existed
- Initial governance canon was present
- Agent roles were conceptually defined
- Execution mechanics could be tested

Based on these assumptions, a readiness certificate was issued.

---

### What Failed in Reality

The platform was **not ready for governed execution** because:

- Governance layer-down into application repositories was incomplete
- PR gate enforcement did not exist
- Agent contracts were insufficiently constrained
- FM pause/resume authority was not structurally enforced
- ‚ÄúReadiness‚Äù had no canonical, auditable definition
- Execution safety depended on human intervention rather than constitutional enforcement

The platform could not sustain governed execution without violating the One-Time Build philosophy.

---

### Root Cause

There was **no governance canon defining what ‚ÄúPlatform Readiness for Build Execution‚Äù means**.

As a result:
- Readiness was declared based on intuition rather than constitutional criteria
- A readiness certificate was issued without enforceable guarantees
- The declaration could not survive contact with real execution

---

### Corrective Outcome

Execution was correctly halted once misalignment was detected.

Rather than retroactively correcting the readiness declaration, this deficiency is captured as a **Bootstrap Learning** in accordance with the ratcheting quality doctrine.

---

### Governance Action Required

This learning mandates the creation of a new governance canon that:

- Defines platform readiness constitutionally
- Specifies mandatory artefacts and enforcement points
- Prevents premature execution in future builds
- Enables deterministic, auditable readiness declarations

This canon applies **only to future builds** and is not retroactively applied to bootstrap execution.

---

### Ratchet Statement

This learning is accepted **once**.

Future platform builds **must not** be initiated without meeting an explicit, canonically defined platform readiness standard.

This condition is now permanently elevated.

---

## BL-010 ‚Äî Platform Readiness Requires Deterministic Validation

### Classification
- **Type:** Governance Learning
- **Phase:** Platform Readiness Reset & Build Initiation Plan (Phase 1.2)
- **Severity:** Critical
- **Status:** Recorded
- **Impacts:** All future platform readiness declarations

---

### Context

Phase 1.2 Platform Readiness Gap Analysis identified that platform readiness canon (G-PLAT-READY-01) defines 6 readiness conditions but lacks deterministic, executable validation methods.

---

### Observed Issue

Platform readiness conditions use terms like `governance_completeness_state()` implying automation, but no such functions exist. Validation depends on human judgment, reintroducing subjectivity the canon was designed to eliminate.

---

### Root Cause

Readiness conditions were defined constitutionally (what must be true) without operational specifications (how to verify truth). Gap between canonical definition and validation implementation.

---

### Learning

Platform readiness conditions **MUST** be accompanied by deterministic validation methods with explicit evidence requirements. Readiness cannot be verified subjectively.

**Validation Requirements**:
- Each readiness condition MUST include a validation method specification
- Validation methods MUST be executable or have explicit manual procedures
- Evidence schemas MUST exist for all conditions
- "Operational" vs "defined" distinction MUST be explicit with test criteria
- Evidence MUST demonstrate enforcement occurred, not just that enforcement could occur

---

### Why This Gap Allowed Failure

Without deterministic validation, readiness declarations depend on evaluator interpretation. Different evaluators may reach different conclusions using identical artifacts. This permits premature readiness declaration based on incomplete validation ‚Äî the exact failure mode BL-009 documented.

---

### Governance Action Required

This learning mandates updates to Platform Readiness Canon (G-PLAT-READY-01) to:
- Define deterministic validation methods for all 6 readiness conditions
- Distinguish "defined" from "operational" with evidence requirements
- Require enforcement evidence, not just existence evidence
- Establish evidence schemas for all conditions
- Specify validation procedures (automated or manual with explicit steps)

---

### Status

**Recorded** ‚Äî Non-Retroactive  
**Applies To:** All future platform readiness declarations  
**Effective:** 2025-12-31

---

## BL-011 ‚Äî Platform Readiness Must Distinguish Repository Scope

### Classification
- **Type:** Governance Learning
- **Phase:** Platform Readiness Reset & Build Initiation Plan (Phase 1.2)
- **Severity:** Critical
- **Status:** Recorded
- **Impacts:** All future platform readiness declarations

---

### Context

Phase 1.2 Platform Readiness Gap Analysis identified that platform readiness canon (G-PLAT-READY-01) evaluates "the platform" without distinguishing governance repository readiness from application repository readiness.

---

### Observed Issue

Platform readiness can be declared for governance repository while application repositories remain uninitialized, ungated, or without enforced contracts. No specification of which repositories must be ready for "platform readiness" to be true.

---

### Root Cause

Platform readiness was conceived as singular state but platform is multi-repository ecosystem. Canon does not specify whether readiness applies per-repository or ecosystem-wide.

---

### Learning

Platform readiness **MUST** specify scope: governance repository only, specific application repository, or ecosystem-wide. Different scopes have different readiness criteria.

**Scope Requirements**:
- Readiness declarations MUST include explicit scope (which repository/repositories)
- Per-repository readiness criteria MUST be explicit
- Ecosystem-wide readiness MUST aggregate per-repository states
- Layer-down completeness MUST be validated per-repository
- Governance repository readiness ‚â† application repository readiness
- Build execution authority tied to build target repository readiness, not governance repository readiness

---

### Why This Gap Allowed Failure

Governance repository can be "ready" while build target repositories are not. Declaring "platform ready" based solely on governance repo state permits build initiation in unprepared repositories ‚Äî the failure mode BL-009 identified.

---

### Governance Action Required

This learning mandates updates to Platform Readiness Canon (G-PLAT-READY-01) to:
- Define readiness scope categories (governance-layer, per-repository, ecosystem-wide)
- Specify which scope applies to each readiness condition
- Distinguish governance repository readiness from application repository readiness
- Require layer-down validation per target repository before build authorization
- Clarify that governance repository readiness is prerequisite, not sufficient condition

---

### Status

**Recorded** ‚Äî Non-Retroactive  
**Applies To:** All future platform readiness declarations  
**Effective:** 2025-12-31

---

## BL-012 ‚Äî AMBER Readiness Requires Explicit Exception Criteria

### Classification
- **Type:** Governance Learning
- **Phase:** Platform Readiness Reset & Build Initiation Plan (Phase 1.2)
- **Severity:** Critical
- **Status:** Recorded
- **Impacts:** All future AMBER readiness declarations

---

### Context

Phase 1.2 Platform Readiness Gap Analysis identified that platform readiness canon (G-PLAT-READY-01) defines AMBER state as "core conditions satisfied but optional elements incomplete" without defining which conditions are core vs optional.

---

### Observed Issue

All 6 readiness conditions are presented as equally mandatory, but AMBER state introduces "optional elements" without specification. AMBER can be used to bypass required conditions by reinterpreting them as optional.

---

### Root Cause

Canon treats all conditions as required but permits AMBER declaration without explicit criteria for when degradation is acceptable. No enumeration of which elements may be incomplete under AMBER.

---

### Learning

AMBER readiness state **MUST** include explicit, non-subjective criteria for which conditions may be incomplete and under what circumstances. "Optional elements" must be enumerated constitutionally, not determined per-declaration.

**AMBER Requirements**:
- Readiness canon MUST classify conditions as REQUIRED vs DEGRADABLE
- AMBER authorization criteria MUST be explicit (not "human judgment" alone)
- Risk thresholds for AMBER MUST be defined (acceptable vs unacceptable degradation)
- AMBER remediation requirements MUST be time-bound and enforceable
- AMBER MUST NOT permit critical enforcement gaps (gates, contracts, STOP mechanics)
- AMBER justification MUST cite specific enumerated exception case

---

### Why This Gap Allowed Failure

Without explicit AMBER criteria, any condition can be retroactively classified as "optional" to permit AMBER declaration. This is functionally equivalent to not having the condition at all ‚Äî the exact failure mode BL-009 warned against.

---

### Governance Action Required

This learning mandates updates to Platform Readiness Canon (G-PLAT-READY-01) to:
- Classify each condition as REQUIRED (must be TRUE for any readiness) or DEGRADABLE (may be incomplete under AMBER with justification)
- Define explicit AMBER exception cases (e.g., "continuous monitoring deferred but manual audit scheduled")
- Prohibit AMBER for core enforcement mechanisms (gates, contracts, STOP authority)
- Require time-bound remediation plans for AMBER declarations
- Establish AMBER review cadence (must transition to GREEN or escalate)

---

### Status

**Recorded** ‚Äî Non-Retroactive  
**Applies To:** All future AMBER readiness declarations  
**Effective:** 2025-12-31

---

## BL-013 ‚Äî Platform Readiness Must Model Progressive Activation

### Classification
- **Type:** Governance Learning
- **Phase:** Platform Readiness Reset & Build Initiation Plan (Phase 1.2)
- **Severity:** Critical
- **Status:** Recorded
- **Impacts:** All future platform readiness models and declarations

---

### Context

Phase 1.2 Platform Readiness Gap Analysis identified that platform readiness canon (G-PLAT-READY-01) treats readiness as binary (GREEN/RED/AMBER) but bootstrap learnings demonstrate platform capabilities activate progressively.

---

### Observed Issue

Current canon defines only one readiness state: "Platform ready for governed build execution." No distinction between readiness for manual execution, delegated execution, supervised execution, and autonomous execution.

---

### Root Cause

Platform readiness was defined monolithically but platform capabilities mature through phases (governance ready ‚Üí FM ready ‚Üí builder ready ‚Üí autonomous execution ready). Binary readiness model cannot represent partial capability readiness.

---

### Learning

Platform readiness **MUST** be modeled as progressive activation with explicit capability-based readiness stages. Different execution modes have different readiness prerequisites.

**Progressive Activation Requirements**:
- Readiness canon MUST define activation stages with explicit prerequisites
- Each stage MUST have explicit capability boundaries
- Readiness declarations MUST specify which activation stage is ready
- Higher activation stages MUST require all lower stage prerequisites
- Stage transitions MUST be explicit and auditable

**Activation Stages** (minimum model):
1. **Governance-Layer Ready**: Governance canon locked, completeness GREEN, layer-down complete in governance repository
2. **Repository Ready**: Application repository initialized, governance seeded, enforcement infrastructure present
3. **Manual Execution Ready**: Architecture frozen, QA-to-red complete, human proxy can execute builds
4. **Delegated Execution Ready**: FM operational, builders recruited, FM-instructs-human-executes viable
5. **Supervised Execution Ready**: Automated execution possible, human oversight active, halt authority proven
6. **Autonomous Execution Ready**: Full automation operational, continuous monitoring active, no human intervention required

---

### Why This Gap Allowed Failure

Binary readiness forces declaring "ready" when platform is only ready for manual execution, or declaring "not ready" when manual execution is viable. BL-009 occurred because platform was "ready enough" for some activities but not others ‚Äî binary model cannot represent this nuance.

---

### Governance Action Required

This learning mandates updates to Platform Readiness Canon (G-PLAT-READY-01) to:
- Define progressive activation stage model
- Map each readiness condition to relevant activation stages
- Specify which stage declarations are valid (not all stages may be declarable immediately)
- Clarify that "Platform Ready for Governed Build Execution" implies a specific activation stage
- Integrate progressive activation with existing System Commissioning and Progressive Activation Protocol

---

### Status

**Recorded** ‚Äî Non-Retroactive  
**Applies To:** All future platform readiness models  
**Effective:** 2025-12-31

---

## BL-014 ‚Äî "Operational" Requires Evidence of Enforcement, Not Just Existence

### Classification
- **Type:** Governance Learning
- **Phase:** Platform Readiness Reset & Build Initiation Plan (Phase 1.2)
- **Severity:** Critical
- **Status:** Recorded
- **Impacts:** All future operational readiness validations

---

### Context

Phase 1.2 Platform Readiness Gap Analysis identified that platform readiness canon (G-PLAT-READY-01) requires governance to be "operational" but does not distinguish between "governance defined" and "governance enforced."

---

### Observed Issue

Multiple conditions use "operational," "active," "enforceable" without specifying evidence threshold. Governance existence (files present) conflated with governance enforcement (rules prevent violations). No test for "has governance actually blocked a violation?" vs "would governance theoretically block a violation?"

---

### Root Cause

Canon requires governance to be "operational" without defining what evidence proves operational status vs merely defined status. Permits declaring readiness when governance exists but has never enforced.

---

### Learning

"Operational" governance **MUST** be proven through enforcement evidence, not existence evidence. Readiness requires proof governance has enforced (past tense), not proof governance could enforce (conditional).

**Operational Evidence Requirements**:
- "Operational" MUST be defined as "has successfully enforced at least once" OR "enforcement mechanism tested and proven"
- Readiness MUST require enforcement test results, not just policy documents
- PR gates MUST have blocked at least one non-compliant PR (or test PR) as evidence they are operational
- STOP mechanics MUST have halted execution (or test halt succeeded) as evidence they are enforceable
- Branch protection MUST be verified programmatically (API check), not visually
- "Active" MUST mean "currently enforcing," not "configured but untested"

**Enforcement Test Requirements**:
- Before declaring operational, governance enforcement MUST be tested
- Tests MAY be synthetic (deliberate violation to prove blocking) if no natural violations occurred
- Test evidence MUST be documented and auditable
- Untested enforcement MUST be classified as "defined but not operational"

---

### Why This Gap Allowed Failure

BL-009 stated: "Execution safety depended on human intervention rather than constitutional enforcement." Governance existed but was not proven to enforce. Declaring readiness based on governance existence without enforcement evidence permits the exact failure mode: "A readiness certificate could be issued without guaranteeing governed execution."

---

### Governance Action Required

This learning mandates updates to Platform Readiness Canon (G-PLAT-READY-01) to:
- Define "operational" as "enforcement proven through evidence"
- Require enforcement test evidence for all operational claims
- Distinguish "defined" (exists) from "operational" (enforces) from "enforced" (has enforced)
- Specify test procedures for proving enforcement (synthetic violations acceptable)
- Require programmatic verification where possible (API checks vs manual inspection)
- Update evidence schemas to capture enforcement test results

---

## BL-015 ‚Äî Architecture Must Be Wiring-Complete to Support One-Time Builds

### Classification
- **Type:** Architecture / Governance Learning
- **Phase:** Phase 4.3 ‚Üí Phase 4.4 Transition
- **Severity:** Catastrophic (One-Time Build Violation)
- **Status:** Recorded
- **Impacts:** All future architecture definitions and QA-to-Red derivations

---

### Context

During Phase 4.3 (Architecture Definition) review, the architecture was found to be
structurally complete, fully traceable to requirements, and compliant with all
formal acceptance criteria.

However, a deeper inspection revealed that the architecture **did not guarantee**
a fully functional, one-time build application.

---

### Observed Issue

The architecture permitted:
- summary-level component definitions without explicit wiring
- implicit assumptions about component contracts
- reliance on builder interpretation to ‚Äúfill in the gaps‚Äù
- QA derivation without ensuring runtime completeness

As a result, it was possible to:
- design a QA-to-Red suite
- recruit builders
- and still produce a *hollow build* (structure without behavior)

This failure mode is analogous to ‚Äúa TV with buttons and casing, but no internal wiring.‚Äù

---

### Root Cause

Architecture completeness was evaluated using:
- structural coverage
- requirement traceability
- governance alignment

‚Ä¶but **not** evaluated against a stricter criterion:

> ‚ÄúCould this architecture, if implemented exactly as written, produce a fully
> functional application in a single build pass without interpretation?‚Äù

No canonical requirement existed that architecture must be **wiring-complete**.

---

### Learning

For a governed, one-time build system:

**Architecture MUST be wiring-complete, not just structurally complete.**

This means:
- No architectural element may exist without explicit operational definition
- All component contracts must be explicit (inputs, outputs, dependencies, failures)
- All runtime paths must be fully described end-to-end
- No reliance on ‚Äúhigh-level‚Äù or ‚Äúsummary-only‚Äù descriptions is permitted
- Architecture must independently guarantee functional completeness

Traceability alone is insufficient.

---

### Governance Impact

This learning mandates that future architecture definitions:

- Are evaluated against a **wiring-completeness standard**
- Explicitly prohibit ‚Äúsummary-only‚Äù architecture sections
- Require every architectural unit to map to numbered QA components
- Ensure QA-to-Red cannot be defined without architectural wiring being complete

This learning directly informs:
- Architecture acceptance criteria
- QA-to-Red design requirements
- Builder task derivation rules

---

### Corrective Action Pattern

When wiring incompleteness is detected:
1. The issue must be classified as **catastrophic**
2. A formal FL/CI root cause analysis must be performed
3. Architecture must be corrected before QA-to-Red is finalized
4. Existing QA artifacts must be re-aligned, not silently patched

---

### Ratchet Statement

We do not accept architectures that *appear complete*.  
We accept only architectures that *cannot produce hollow builds*.

This condition is now permanently elevated.

---

### Status

**Recorded** ‚Äî Non-Retroactive  
**Applies To:** All future operational readiness validations  
**Effective:** 2025-12-31

---

## BL-016 ‚Äî FM Must Automatically Recognize and Respond to Execution Complexity Limits

### Classification
- **Type:** Governance Learning
- **Phase:** Wave 1.0.7 Execution Observation
- **Severity:** Structural
- **Status:** Recorded
- **Impacts:** FM Authority, Escalation Requirements, Execution Safety Model

---

### Context

During Wave 1.0.7 execution, real-world platform and cognitive constraints surfaced that tested the system's ability to self-recognize execution pressure without external prompting.

Bootstrap execution revealed that FM operates under practical constraints:
- Model context limits
- Platform execution time limits
- Cognitive capability boundaries
- Multi-step execution complexity thresholds

---

### Observed Issue

Existing governance defines:
- **When FM must escalate** (governance conflicts, unrecoverable failures, security concerns)
- **What conditions trigger hard stops** (CS violations, circuit breaker thresholds, catastrophic failures)
- **FM's authority to pause/halt for specific trigger conditions**

However, governance does NOT explicitly define:
- FM's obligation to self-recognize when execution complexity exceeds practical capability
- FM's authority to halt or pause **based on complexity/pressure assessment** (before hitting specific failure thresholds)
- FM's responsibility to filter execution pressure signals through FM judgment
- FM's expected behavior when task complexity approaches cognitive/platform limits

---

### Root Cause

FM escalation rules were defined for **specific failure conditions** (QA failures, governance violations, security incidents) but not for **general execution complexity assessment**.

The system implicitly assumed FM would:
- Recognize when a task exceeds reasonable execution capability
- Self-assess complexity vs. available resources (time, context, platform limits)
- Halt or pause execution proactively if complexity is impractical
- Do so without requiring external instruction or specific failure triggers

This expectation was **never explicitly canonized**.

---

### Learning

**When execution complexity exceeds practical capability** (model limits, context saturation, platform constraints, cognitive overload), **FM is expected to automatically recognize this condition, halt or pause execution if required, and re-evaluate execution strategy through FM authority** ‚Äî even if this temporarily stops the build.

**This is NOT about automatic correction or enforcement.**  
**This is about automatic recognition and self-filtering by FM.**

**Key Characteristics**:
1. **Self-Recognition**: FM must detect when execution is approaching or exceeding practical limits
2. **FM Judgment**: FM evaluates whether complexity is manageable or requires intervention
3. **Proactive Halt/Pause**: FM may halt or pause execution **before** hitting specific failure thresholds
4. **Re-Evaluation**: FM re-evaluates execution strategy (decomposition, delegation, escalation to human)
5. **Authority Preservation**: FM retains decision-making authority; this is not automated enforcement

**Complexity Indicators** (non-exhaustive):
- Task requires reasoning beyond available context window
- Execution timeline exceeds platform limits (GitHub Actions timeouts)
- Multi-step orchestration complexity approaches cognitive saturation
- Dependency chains create unmaintainable execution state
- QA validation becomes impractically large or complex
- Architecture completeness requires iteration beyond practical bounds

**FM Response Options**:
- **Pause and Decompose**: Break complex task into smaller sub-tasks
- **Escalate for Simplification**: Request human authority to simplify requirements
- **Halt for Resource Allocation**: Halt execution until platform/cognitive resources available
- **Re-Plan Execution Strategy**: Adjust approach (phased execution, delegation, staged delivery)

---

### Why This Gap Allowed Pressure

Without explicit governance stating FM must self-recognize execution complexity:
- FM may proceed with impractical execution attempts (exhausting resources without outcome)
- FM may interpret "continue execution" mandate as "never halt for complexity"
- Human authority may need to externally observe and intervene (reactive, not proactive)
- Execution pressure accumulates until specific failure thresholds trigger (late detection)

Explicit governance enables FM to **proactively manage execution complexity** rather than **reactively respond to failures**.

---

### Governance Impact

This learning establishes:
- FM has **obligation** to self-recognize execution complexity limits
- FM has **authority** to halt/pause based on complexity assessment (not just specific failures)
- FM **judgment** is authoritative for complexity evaluation (with escalation if uncertain)
- Execution safety includes **proactive complexity management**, not just reactive failure response

**Integration with Existing Canon**:
- Extends **FOREMAN_AUTHORITY_AND_SUPERVISION_MODEL.md** escalation boundaries (¬ß7.2 Soft Stop Conditions) to include complexity assessment
- Aligns with **BUILD_INTERVENTION_AND_ALERT_MODEL.md** Pause semantics (¬ß3.4) for resource/strategic decisions
- Complements **CASCADING_FAILURE_CIRCUIT_BREAKER.md** by enabling halt **before** failure accumulation
- Integrates with **COGNITIVE_CAPABILITY_ORCHESTRATION_MODEL.md** by recognizing cognitive capability limits

---

### Governance Action Required

This learning informs future updates to:
- **FOREMAN_AUTHORITY_AND_SUPERVISION_MODEL.md**: Add "Execution Complexity Assessment" as soft stop condition (¬ß7.2)
- **BUILD_INTERVENTION_AND_ALERT_MODEL.md**: Clarify Pause authority includes complexity-based pauses (¬ß3.4)
- **FM_ROLE_CANON.md**: Add "Self-Recognize Complexity Limits" to core responsibilities (¬ß7)

These updates are **informational**, not immediately required. This learning captures the expectation for future canonization.

---

### Status

**Recorded** ‚Äî Non-Retroactive  
**Applies To:** All FM execution going forward  
**Effective:** 2026-01-03

---

### Ratchet Statement

FM is not expected to execute beyond practical capability.

FM is expected to self-recognize execution pressure and respond through FM authority.

This expectation is now explicitly recorded.

---


---

## BL-016 ‚Äî AI Escalation and Capability Orchestration Must Be Operationally Active, Not Theoretical

### Classification
- **Type:** Governance Activation (Bootstrap-to-Canon Ratchet)
- **Phase:** Wave 1.0.7 Stress Analysis
- **Severity:** Critical (Silent Governance Gap)
- **Status:** Recorded ‚Äî Activation Complete
- **Impacts:** All future FM execution, escalation decisions, and capability orchestration

---

### Context

During high-complexity execution (Wave 1.0.7), AI escalation and cognitive capability orchestration mechanisms existed in governance canon but **were not operationally activated**.

Governance defined:
- ESCALATION_POLICY.md (hierarchical authority levels L1-L4)
- COGNITIVE_CAPABILITY_ORCHESTRATION_MODEL.md (capability classes: reasoning, coding, analysis, etc.)
- FM escalation responsibilities (reactive, failure-triggered)

However:
- Escalation was **reactive only** (after failures), not **proactive** (before cognitive limits exceeded)
- Capability orchestration was **planning-only**, not **execution-relevant**
- FM had **no explicit halt semantics** for cognitive limit awareness
- Complexity-aware capability scaling was **not defined as binding FM behavior**

---

### Observed Issue

Wave 1.0.7 execution stress revealed:
- High-complexity task did not trigger proactive escalation
- Cognitive capability orchestration existed but was not integrated with escalation triggers
- FM proceeded per governance (escalate only after failures)
- Expectation: FM would escalate proactively based on complexity
- Reality: Proactive escalation was **not defined in governance**

**Root Cause**: Governance mechanisms existed but were **non-operational**. Humans implicitly relied upon escalation and capability scaling during Wave 1.0.7, but governance did not define these behaviors as **binding and active**.

This created a **silent governance gap** where escalation and capability orchestration were **expected in practice but non-binding in governance**.

---

### Learning

**Governance mechanisms that exist on paper but are not operationally activated are forbidden in a One-Time Build system.**

If governance defines escalation and capability orchestration:
- These mechanisms **MUST** be explicitly activated as **operational, binding behavior**
- FM **MUST** be expected to invoke, assess, and decide based on these mechanisms
- Escalation **MUST** include proactive triggers (complexity-aware), not just reactive triggers (failure-based)
- Capability orchestration **MUST** be execution-relevant, not just planning-relevant
- FM **MUST** have explicit halt semantics when cognitive limits are reached

**Theoretical governance creates false security**. If a mechanism is defined but not activated, humans assume it will engage when needed, but agents correctly follow only what is **explicit and binding**.

---

### Corrective Action (2026-01-03 Activation)

This learning mandates **immediate activation** of AI escalation and cognitive capability orchestration:

1. **ESCALATION_POLICY.md** amended to:
   - Activate proactive, complexity-aware escalation (not just reactive, failure-based)
   - Define explicit triggers: task complexity exceeds capability, cognitive saturation detected, no suitable capability class
   - Integrate with cognitive capability orchestration model
   - Redefine "escalation as failure signal" to include **proactive escalation as expected behavior**

2. **COGNITIVE_CAPABILITY_ORCHESTRATION_MODEL.md** amended to:
   - Activate as **operational governance** (not planning-only)
   - Add Section 5.5: Complexity-Aware Capability Scaling (ACTIVE)
   - Add Section 5.5.1: Explicit FM Halt Semantics for Cognitive Limits (ACTIVE)
   - Define mandatory FM responsibilities: assess complexity, select capability, switch when needed, halt when limits exceeded
   - Define audit requirements for all complexity assessments and capability selections

3. **FM_ROLE_CANON.md** amended to:
   - Add Section 9: Cognitive Capability Orchestration (ACTIVE)
   - Add Section 10: Explicit Halt Semantics for Cognitive Limits (ACTIVE)
   - Update Section 8: Escalation to include proactive, complexity-aware triggers
   - Define halt as **proactive, non-punitive, expected behavior** (not failure)

4. **BOOTSTRAP_EXECUTION_LEARNINGS.md** updated to:
   - Record this activation as BL-016 (bootstrap-to-canon ratchet)
   - Classify as critical governance gap revealed by execution stress
   - Document that activation makes mechanisms **operationally binding**, not theoretical

---

### Ratchet Statement

**This activation is permanent and non-retroactive.**

Going forward:
- AI escalation **MUST** be operational, not theoretical
- Cognitive capability orchestration **MUST** be execution-relevant, not planning-only
- FM **MUST** proactively assess complexity and escalate/halt when limits are reached
- Governance mechanisms that exist **MUST** be explicitly activated as binding behavior

**Theoretical governance that is not activated is governance debt.**

We do not accept governance mechanisms that exist on paper but are not operationally binding.

This condition is now permanently elevated.

---

### Status

**Recorded** ‚Äî Non-Retroactive  
**Applies To:** All future FM execution, escalation decisions, and capability orchestration  
**Effective:** 2026-01-03

---

### Cross-References

- Issue: üî¥ ACTIVATE AI Escalation & Capability-Aware Scaling (Governance Activation)
- Survey: AI_ESCALATION_CAPABILITY_SCALING_SURVEY_AND_RCA.md
- Amended Artifacts:
  - governance/escalation/ESCALATION_POLICY.md (v2.0 ‚Äî ACTIVE)
  - governance/canon/COGNITIVE_CAPABILITY_ORCHESTRATION_MODEL.md (v1.1.0 ‚Äî ACTIVE)
  - governance/maturion/FM_ROLE_CANON.md (updated 2026-01-03)

---

## BL-017 ‚Äî Execution Progress Must Be Systematically Recorded in a Single Canonical Artifact

### Classification
- **Type:** Governance Learning (Execution Integrity)
- **Phase:** Wave 1 Execution Observation
- **Severity:** Critical (Auditability and Wave Closure Risk)
- **Status:** Recorded
- **Impacts:** All future wave executions, FM responsibilities, wave closure certification

---

### Context

During Wave 1 execution in the FM App repository, a structural gap was identified at Wave closure:

- Execution progress was distributed across multiple long-lived and unstable PRs
- Some FM-referenced artifacts could not be reliably located
- Wave completion could not be certified based on a single authoritative progress record
- Progress knowledge relied partially on implicit memory rather than explicit evidence

This was surfaced during Wave 1 gate merge preparation and required progress reconstruction before wave closure could be validated.

---

### Observed Issue

Current governance **does not explicitly require**:

- A single canonical execution progress record
- Systematic artifact indexing (what exists, where, status)
- Explicit wave closure certification based on evidence
- A defined FM obligation to reconstruct progress when execution context spans multiple PRs

As a result:
- Progress can exist without being canonically recorded
- Wave closure risks being implicit rather than evidence-based
- Auditability and repeatability are weakened
- FM cannot certify wave completion from evidence alone

---

### Root Cause

**Governance gap**: No canon explicitly mandates that:

1. FM MUST maintain a canonical progress record per wave
2. The record MUST be authoritative over memory, PR history, and chat context
3. Progress MUST be recorded systematically (per phase, per artifact)
4. Wave closure MUST be certified based on evidence, not assumption

The One-Time Build system requires explicit evidence at all stages, but wave progress recording was implicitly assumed rather than constitutionally mandated.

---

### Learning

**For a governed, one-time build system with auditable wave execution:**

**Execution progress MUST be systematically and completely recorded in a single canonical progress artifact, suitable for audit and wave closure certification.**

This means:
- FM MUST maintain a canonical progress record per wave (e.g., `WAVE_<n>_IMPLEMENTATION_PROGRESS.md`)
- The progress record is authoritative over all other sources (memory, PR history, conversations)
- Progress MUST be recorded per wave and per phase
- Artifact index MUST be explicit (name ‚Üí path ‚Üí status)
- Wave closure certification MUST be based on evidence review, not assumption
- FM MUST reconstruct progress from all execution surfaces when context degrades
- Wave closure without certification is invalid

**Key Characteristics**:
1. **Single Source of Truth**: One canonical progress artifact per wave
2. **Systematic Recording**: Progress recorded per phase (architecture, QA, build, validation)
3. **Artifact Indexing**: All instructed artifacts explicitly tracked (name, path, status)
4. **Evidence-Based Certification**: Wave completion verdict derived from artifact review
5. **Reconstruction Obligation**: FM must reconstruct if progress context degrades
6. **Blocking Authority**: Wave gate merge blocked without certified completion

---

### Why This Gap Allowed Failure

Without explicit governance stating FM must maintain canonical progress records:
- Progress exists implicitly (in memory, PRs, conversations) but not authoritatively
- Wave closure can be declared based on "I think we're done" rather than "Evidence shows completion"
- Artifact location and status become ambiguous over time
- Auditability degrades as execution context spans multiple PRs or time periods
- Repeatability is impossible (future waves cannot learn from unclear past execution)

Explicit governance enables FM to **proactively maintain progress** rather than **retroactively reconstruct it under pressure**.

---

### Governance Impact

This learning establishes:
- FM has **obligation** to maintain canonical progress record per wave
- FM has **responsibility** to certify wave closure based on evidence
- FM MUST **reconstruct progress** if execution context degrades
- Wave gate merge is **blocked** without certified wave completion

**Integration with Existing Canon**:
- Extends **FOREMAN_AUTHORITY_AND_SUPERVISION_MODEL.md** Control responsibilities (¬ß4.4) to include progress recording
- Extends **FM_ROLE_CANON.md** evidence trail maintenance (¬ß6) to require canonical progress artifact
- Complements **WAVE_MODEL.md** wave completion criteria (¬ßVIII) by requiring explicit certification
- Aligns with **AUDIT_READINESS_MODEL.md** by ensuring wave execution is fully auditable

---

### Governance Action Required

This learning informs creation of new governance canon:
- **MANDATORY_CANONICAL_PROGRESS_RECORDING_AND_WAVE_CLOSURE_CERTIFICATION.md**: Define constitutional requirement for progress recording
- **WAVE_IMPLEMENTATION_PROGRESS.template.md**: Define mandatory progress artifact format
- **WAVE_IMPLEMENTATION_PROGRESS.schema.md**: Define validation requirements for progress artifacts
- Layer-down to **FM_ROLE_CANON.md**: Add progress recording as core FM responsibility
- Layer-down to **FOREMAN_AUTHORITY_AND_SUPERVISION_MODEL.md**: Add to Control (C) phase responsibilities

These updates ensure future waves cannot experience the same progress ambiguity.

---

### Ratchet Statement

**This requirement is permanent and non-retroactive.**

Going forward:
- Wave execution progress **MUST** be recorded in a canonical artifact
- Wave closure **MUST** be certified based on evidence
- FM **MUST** maintain artifact indexes (name ‚Üí path ‚Üí status)
- Wave gate merge **MUST** be blocked without certified completion

**Implicit progress tracking is governance debt.**

We do not accept wave closures that depend on memory or assumption.

This condition is now permanently elevated.

---

### Status

**Recorded** ‚Äî Non-Retroactive  
**Applies To:** All future wave executions  
**Effective:** 2026-01-04

---

### Cross-References

- Issue: #[ISSUE_NUMBER] ‚Äî Mandatory Canonical Progress Recording & Wave Closure Certification
- Implements: MANDATORY_CANONICAL_PROGRESS_RECORDING_AND_WAVE_CLOSURE_CERTIFICATION.md (to be created)
- Template: governance/templates/WAVE_IMPLEMENTATION_PROGRESS.template.md (to be created)
- Schema: governance/schemas/WAVE_IMPLEMENTATION_PROGRESS.schema.md (to be created)
- Layer-Down Impacts:
  - governance/maturion/FM_ROLE_CANON.md
  - governance/canon/FOREMAN_AUTHORITY_AND_SUPERVISION_MODEL.md
  - governance/execution/WAVE_MODEL.md

---

## BL-018 ‚Äî Wave Planning MUST Verify QA Catalog Before Subwave Assignment (Cross-Repo Canonical Reference)

### Classification
- **Type:** Governance Learning (Architecture and Planning Integrity)
- **Phase:** Wave 2.2 Execution (FM Office App)
- **Severity:** Catastrophic (First-Time Failure)
- **Status:** Canonized to Platform-Wide Requirements
- **Impacts:** All wave-based delivery systems, all applications

---

### Context

**Source Repository**: `maturion-foreman-office-app`  
**Date Registered**: 2026-01-05  
**Issue Reference**: #399 (Wave 2.2 Block ‚Äî Parking Station Subwave)

During Wave 2.2 (Parking Station Advanced) planning and execution in the FM Office App, a catastrophic planning gap was discovered: QA ranges were assigned to subwaves without verifying that the QA components existed in the canonical QA Catalog or that their semantic definitions matched the intended feature scope.

---

### Observed Issue

Wave 2.2 was planned with QA-376 to QA-385 as the assigned QA range for parking station features (prioritization and bulk operations). However, these QA IDs in `QA_CATALOG.md` were actually:
- **QA-376 to QA-380**: Network Failure Modes (network partition, WebSocket loss, API timeout, GitHub API failure, notification failure)
- **QA-381 to QA-385**: Resource Failure Modes (memory exhaustion, CPU overload, disk space, file handle exhaustion, thread pool exhaustion)

**Complete semantic disconnect**: Parking station UI features vs network/resource failure modes.

**Impact**:
- Wave 2.2 subwave specification was structurally invalid
- Builder (ui-builder) would have been assigned failure mode tests instead of UI features
- Issue #398 created with non-existent QA components as scope
- Wave 2 execution blocked at subwave 2.2

---

### Root Cause

Wave 2 planning proceeded without verifying:
- QA component existence in `QA_CATALOG.md`
- QA definition semantic alignment with subwave intent
- Architectural sequence: Architecture ‚Üí QA Catalog ‚Üí QA-to-Red ‚Üí Planning

**Governance Failure**: No validation step existed to ensure QA Catalog alignment before sub-issue creation.

---

### Learning

**Wave planning and subwave assignment MUST verify that all assigned QA ranges exist in the canonical QA Catalog and semantically match the intended feature scope.**

QA ranges cannot be assumed or assigned sequentially without validation. The canonical flow must be enforced:

```
Architecture ‚Üí QA Catalog Extension ‚Üí QA-to-Red Creation ‚Üí Wave Planning ‚Üí Subwave Assignment
```

No skipping allowed.

---

### Canonization (Platform-Wide)

This learning has been **canonized into governance** and applies to **ALL repositories**:

**Canon Updates**:
1. **ARCHITECTURE_COMPLETENESS_REQUIREMENTS.md ¬ß 3.14**
   - QA Catalog Alignment and Validation (BL-018/BL-019-Derived)
   - Mandatory QA-CATALOG-ALIGNMENT-GATE validation checklist
   - Prohibited actions: Planning without catalog verification

2. **QA_CATALOG_ALIGNMENT_GATE_CANON.md** (NEW)
   - Canonical gate definition for QA Catalog alignment
   - 5 mandatory validations: Existence, Semantic, Collision, Architecture, QA-to-Red
   - Automation patterns and CI/CD integration

3. **LEARNING_INTAKE_AND_PROMOTION_MODEL.md ¬ß 6.3**
   - BL Forward-Scan obligation established
   - Mandatory after ANY BL creation

4. **BUILD_PHILOSOPHY.md v1.3**
   - Second-time failure prohibition elevated to anti-pattern

**Evidence**: `governance/evidence/BL_018_019_CANONIZATION_EVIDENCE_SUMMARY.md`

---

### Mandatory Requirements (Permanent, Platform-Wide)

All future wave planning and subwave assignment MUST include:

1. **QA Catalog Verification**: Before assigning QA ranges, verify all QA IDs exist in canonical QA Catalog
2. **QA Definition Alignment**: Verify QA component definitions match intended feature scope
3. **QA ID Collision Check**: Verify assigned ranges not already allocated to other features
4. **Architecture Completeness**: Verify architecture sections exist for all subwave features
5. **QA Catalog Extension (If Needed)**: Extend catalog BEFORE wave planning if new features require it
6. **Sequential Governance**: Architecture ‚Üí QA Catalog ‚Üí QA-to-Red ‚Üí Wave Planning (in order)

---

### Prohibited Actions (Permanent, Platform-Wide)

- ‚ùå Assigning QA ranges without verifying QA_CATALOG.md
- ‚ùå Assuming QA components exist based on sequential numbering
- ‚ùå Planning waves before architecture extended with new features
- ‚ùå Creating sub-issue specs without QA Catalog validation
- ‚ùå Skipping QA-to-Red precondition verification
- ‚ùå Allowing builders to proceed with invalid QA assignments

---

### Ratchet Statement

**This learning establishes that wave planning without QA Catalog verification is a catastrophic structural failure requiring complete rework.**

This is a **first-time failure** (CATASTROPHIC classification) but expected learning opportunity. The system has learned and canonized prevention mechanisms.

**Second occurrences trigger EMERGENCY classification and TARP activation** (see BL-019).

---

### Status

**Recorded & Canonized** ‚Äî Platform-Wide, Non-Retroactive  
**Applies To:** All repositories with wave-based delivery  
**Effective:** 2026-01-05

---

### Cross-References

**FM Office App (Source)**:
- `BOOTSTRAP_EXECUTION_LEARNINGS.md` ‚Äî BL-018 detailed entry
- `FLCI_REGISTRY_UPDATE_BL_018.md` ‚Äî FL/CI registry entry
- `ROOT_CAUSE_ANALYSIS_WAVE_2_2_BLOCK.md` ‚Äî Detailed RCA
- `WAVE_2_EXECUTION_RATCHET_QA_CATALOG_VERIFICATION.md` ‚Äî Ratchet checklist

**Governance Repo (Canonical)**:
- `governance/canon/ARCHITECTURE_COMPLETENESS_REQUIREMENTS.md` ¬ß 3.14
- `governance/canon/QA_CATALOG_ALIGNMENT_GATE_CANON.md`
- `governance/canon/LEARNING_INTAKE_AND_PROMOTION_MODEL.md` ¬ß 6.3
- `BUILD_PHILOSOPHY.md` v1.3 ‚Äî Second-time failure prohibition
- `governance/evidence/BL_018_019_CANONIZATION_EVIDENCE_SUMMARY.md`

---

## BL-019 ‚Äî Second-Time QA Catalog Semantic Misalignment and Forward-Scan Failure (Cross-Repo Canonical Reference)

### Classification
- **Type:** Governance Learning (Enforcement Failure ‚Äî Second-Time)
- **Phase:** Wave 2.3+ Execution (FM Office App)
- **Severity:** Emergency (Second-Time Failure ‚Äî TARP Activation)
- **Status:** Canonized to Platform-Wide Requirements
- **Impacts:** All BL/FL/CI processes, all forward-scan obligations, all wave-based delivery

---

### Context

**Source Repository**: `maturion-foreman-office-app`  
**Date Registered**: 2026-01-05 (SAME DAY as BL-018)  
**Issue Reference**: #402 (Subwave 2.3 Invalid Appointment), PR #403 (Builder Rejection)

After BL-018 was documented for Wave 2.2 QA Catalog misalignment, FM created a ratchet but **failed to forward-scan remaining Wave 2 subwaves (2.3 to 2.14)** for the same pattern. When Subwave 2.3 was issued, api-builder correctly applied BL-018 verification and rejected the appointment as INVALID due to the **exact same QA Catalog misalignment pattern**.

**This is a SECOND-TIME FAILURE** of the same pattern on the same day.

---

### Observed Issue

**Subwave 2.3 Assignment**:
- Claimed: QA-341 to QA-350 for "System Optimizations Phase 1" (Caching, Query Optimization)
- Actual Catalog: Analytics/Memory/Storage/Logging/Watchdog Failure Modes
- **Complete semantic disconnect** (same pattern as BL-018)

**Forward-Scan Results** (performed after second failure):
- **9 of 14 Wave 2 subwaves (64%)** affected by same pattern
- Misaligned: 2.1, 2.2, 2.3, 2.6, 2.9, 2.10
- Undefined: 2.4, 2.13, 2.14
- Only 5 subwaves (36%) correctly aligned

**Builder Response**:
- api-builder correctly applied BL-018 ratchet to Subwave 2.3
- Rejected appointment as INVALID
- Declared BLOCKED per governance
- **Governance working correctly; prevention mechanism failed**

---

### Root Cause (Second-Order Failure)

**Primary Failure**: FM failed to apply BL-018 ratchet when it was created

**Failure Sequence**:
1. Wave 2 planned with QA misalignments (BL-018 triggered for 2.2)
2. BL-018 ratchet created with verification checklist
3. **FM did NOT forward-scan remaining Wave 2.3+ subwaves**
4. Wave 2.3 issued without correcting known gap
5. api-builder correctly rejected (governance working, but second occurrence happened)

**This is a SECOND-ORDER FAILURE**:
- **First failure (BL-018)**: Planning without QA verification (catastrophic, expected)
- **Second failure (BL-019)**: Not correcting all instances after discovery (emergency, prohibited)

---

### Learning

**When ANY Bootstrap Learning or FL/CI entry is recorded, the system MUST perform a forward-scan of ALL relevant pending work to identify and correct additional instances of the same failure pattern.**

Recording a single learning without scanning for additional occurrences violates the "never repeat" principle. Forward-scan is mandatory, non-negotiable, and the PRIMARY mechanism for preventing second-time failures.

**Second-time failures trigger EMERGENCY classification and TARP (Trigger Action Response Plan) activation.**

---

### Canonization (Platform-Wide)

This learning has been **canonized into governance** and applies to **ALL repositories**:

**Canon Updates**:
1. **LEARNING_INTAKE_AND_PROMOTION_MODEL.md ¬ß 6.3**
   - BL Forward-Scan Obligation (BL-019-Derived)
   - 5-step mandatory forward-scan process
   - Forward-scan validation questions (all must be YES)
   - Prohibited: Recording BL without forward-scan

2. **BUILD_PHILOSOPHY.md v1.3**
   - No Second-Time Failures (anti-pattern)
   - First-time = CATASTROPHIC (expected learning)
   - Second-time = EMERGENCY (TARP activation)
   - BL-019 documented as example

3. **TARP_SECOND_TIME_FAILURE_TEMPLATE.md** (NEW)
   - Comprehensive TARP structure (5 phases, 0-48 hours)
   - BL-019 as worked example
   - Integration with governance canon

4. **QA_CATALOG_ALIGNMENT_GATE_CANON.md**
   - Failure severity classification updated
   - TARP activation for second-time QA misalignments

**Evidence**: `governance/evidence/BL_018_019_CANONIZATION_EVIDENCE_SUMMARY.md`

---

### Mandatory Forward-Scan Process (Permanent, Platform-Wide)

When a BL is recorded:

1. **Identify Failure Pattern** ‚Äî Extract root cause, define pattern abstractly
2. **Scan ALL In-Scope Pending Work** ‚Äî Not just affected items, ALL relevant work
3. **Validate Each Instance** ‚Äî Apply new learning/ratchet to each item
4. **Correct ALL Instances** ‚Äî Do NOT proceed with only triggering instance
5. **Evidence and Auditability** ‚Äî Document forward-scan, list corrections

---

### Prohibited Actions (Permanent, Platform-Wide)

- ‚ùå Recording BL without performing forward-scan
- ‚ùå Correcting only triggering instance and proceeding with others
- ‚ùå Assuming "other instances are probably fine" without validation
- ‚ùå Deferring forward-scan corrections
- ‚ùå Issuing authorizations before forward-scan complete

---

### TARP Activation (BL-019 Example)

**Phase 1: IMMEDIATE STOP (0-2 hours)**
- Wave 2 execution SUSPENDED
- All pending subwaves (2.3 to 2.14) blocked

**Phase 2: EMERGENCY ASSESSMENT (2-8 hours)**
- Failure pattern: Same as BL-018 (QA Catalog misalignment)
- Why did prevention fail: No forward-scan after BL-018
- Scope: 9 of 14 subwaves affected (64%)

**Phase 3: RAPID CORRECTIVE ACTIONS (8-24 hours)**
- Forward-scan: All 14 Wave 2 subwaves analyzed
- Automation: Created `validate-wave2-qa-alignment.py` (tested, working)
- Governance: BL-019 FL/CI entry, canonization initiated

**Phase 4: SYSTEM-LEVEL CHANGE VERIFICATION (24-48 hours)**
- Structural prevention: QA-CATALOG-ALIGNMENT-GATE mandatory
- Automation: Validation script blocks misalignments (exit 1)
- Governance canon: 6 documents created/updated

**Phase 5: RESUMPTION APPROVAL (48+ hours)**
- Status: PENDING completion of 9 subwave corrections
- Timeline: 8-12 days for full correction
- Approval: FM (after corrections) ‚Üí Owner (after canonization)

---

### Ratchet Statement

**Forward-scan after BL recording is now a mandatory, non-negotiable requirement.**

Failure to forward-scan is a governance violation.

**Second-time failures are EMERGENCIES** requiring TARP activation. Third-time failures must be impossible by design.

---

### Status

**Recorded & Canonized** ‚Äî Platform-Wide, Non-Retroactive  
**Applies To:** All BL/FL/CI processes, all repositories  
**Effective:** 2026-01-05

---

### Cross-References

**FM Office App (Source)**:
- `BOOTSTRAP_EXECUTION_LEARNINGS.md` ‚Äî BL-019 detailed entry
- `BL_019_EXECUTIVE_SUMMARY.md` ‚Äî Second-time failure analysis
- `BL_019_README.md` ‚Äî Investigation package
- `FLCI_REGISTRY_UPDATE_BL_019_SECOND_FAILURE_CATASTROPHIC.md` ‚Äî FL/CI registry
- `WAVE_2_FORWARD_SCAN_QA_ALIGNMENT_VERIFICATION.md` ‚Äî Forward-scan results
- `WAVE_2_EMERGENCY_CORRECTIVE_ACTION_PLAN_BL_019.md` ‚Äî Corrective actions
- `validate-wave2-qa-alignment.py` ‚Äî Validation script
- `wave2-qa-alignment-validation-results.json` ‚Äî Evidence

**Governance Repo (Canonical)**:
- `governance/canon/LEARNING_INTAKE_AND_PROMOTION_MODEL.md` ¬ß 6.3
- `BUILD_PHILOSOPHY.md` v1.3 ‚Äî Second-time failure prohibition
- `governance/templates/TARP_SECOND_TIME_FAILURE_TEMPLATE.md`
- `governance/canon/QA_CATALOG_ALIGNMENT_GATE_CANON.md`
- `governance/evidence/BL_018_019_CANONIZATION_EVIDENCE_SUMMARY.md`

---

**Maintained by**: Maturion Governance Administrator  
**Last Updated**: 2026-01-05  
**Registry Status**: ACTIVE

---

**Next Learning ID**: BL-021

## BL-020 ‚Äî FM Pre-Authorization Structural Failure (Subwave 2.5) (Cross-Repo Canonical Reference)

### Classification
- **Type:** Governance Learning (Structural Failure ‚Äî FM Planning/Authorization Process)
- **Phase:** Wave 2.5 Execution (FM Office App)
- **Severity:** Catastrophic (First-Time) ‚Äî Structural defense mechanism missing
- **Status:** Canonized to Platform-Wide Requirements
- **Impacts:** All FM planning/authorization decisions, all wave/subwave assignments, all builder appointments

---

### Context

**Source Repository**: `maturion-foreman-office-app`  
**Date Registered**: 2026-01-05  
**Issue Reference**: Subwave 2.5 Authorization Failure (Post BL-018/BL-019)

Despite strong governance canon for:
- QA-Catalog-Alignment (BL-018/BL-019)
- BL Forward-Scan Obligation
- Second-time Failure Prohibition (TARP)
- FM and builder `.agent` contracts describing these duties

FM **repeatedly made authorization mistakes at the planning/gating layer** (e.g., Subwave 2.5), indicating a **missing structural defense mechanism** at the FM planning layer.

---

### Observed Issue

**Pattern**: FM does not consistently run an explicit, structured pre-authorization checklist before:
- Declaring subwaves "READY FOR AUTHORIZATION"
- Issuing builder appointments

**Specific Instance (Subwave 2.5)**:
- After BL-018/BL-019 corrections, FM attempted to authorize Subwave 2.5
- Authorization attempted without systematic validation of:
  - QA Catalog alignment (BL-018/BL-019 pattern)
  - QA-to-Red test foundation completeness
  - Architecture alignment with planned scope
  - BL/FL-CI ratchet compliance status
  - Dependency gate satisfaction
- Authorization mistakes occurred despite existing governance canon

---

### Root Cause

**Primary Failure**: FM lacks an **explicit, structured pre-authorization checklist** that acts as a **mandatory gate** before authorization decisions.

**Failure Sequence**:
1. Governance canon exists for QA alignment, BL forward-scan, architecture completeness
2. FM `.agent` contract references these requirements
3. Builder contracts enforce these requirements
4. **BUT**: FM has no systematic checklist forcing validation before authorization
5. Authorization mistakes occur because validation is implicit/optional rather than explicit/mandatory
6. FM authorization decisions lack structural defense mechanism

**Contributing Factors**:
- Governance requirements scattered across multiple canon documents
- No single "gate" document consolidating pre-authorization validation
- FM agent prompt lacks explicit checklist execution requirement
- Authorization readiness validation is implicit rather than enforced
- No standardized format for recording pre-authorization validation

---

### Learning

**Core Insight**: FM authorization without an explicit, structured pre-authorization checklist is a **structural vulnerability** that permits governance failures to enter execution.

**Constitutional Principle**: FM planning/authorization authority MUST be paired with a **mandatory pre-authorization validation mechanism** that forces systematic readiness checking before ANY wave/subwave authorization or builder appointment.

**Required Structural Fix**:

1. **Canonize FM Pre-Authorization Checklist**
   - Define canonical checklist at governance level (`FM_PREAUTH_CHECKLIST_CANON.md`)
   - Mandatory items: QA Catalog Alignment, QA-to-Red Foundation, Architecture Alignment, BL/FL-CI Ratchet Status, Dependency Gates
   - Clear PASS/FAIL outcomes with explicit authorization blocking on FAIL

2. **Ripple to FM Repo**
   - Create FM-local checklist implementation (`governance/specs/FM_PREAUTH_CHECKLIST.md`)
   - Integrate into FM planning/authorization documentation
   - Provide validation tools/scripts where applicable

3. **Layer Down into .agent Binding**
   - Update `ForemanApp-agent.md` with strong reference to checklist
   - Explicit instruction: "Before authorizing waves/subwaves, you MUST execute FM_PREAUTH_CHECKLIST and report PASS/FAIL for each item"
   - Enforcement language: Checklist execution non-negotiable

4. **Runtime Enforcement**
   - FM executes checklist before EVERY authorization
   - Record checklist execution evidence in wave/subwave planning documents
   - Checklist FAIL blocks authorization absolutely
   - Forward-scan after BL/FL-CI includes checklist re-execution

---

### Governance Impact

**New Canonical Standard Created**:
- `governance/canon/FM_PREAUTH_CHECKLIST_CANON.md` ‚Äî Comprehensive FM pre-authorization checklist

**Existing Canon Updated**:
- `ARCHITECTURE_COMPLETENESS_REQUIREMENTS.md` ¬ß 3.14 ‚Äî Reference to FM Pre-Authorization Checklist
- `LEARNING_INTAKE_AND_PROMOTION_MODEL.md` ¬ß 6.3 ‚Äî Forward-scan integrated into checklist
- `BOOTSTRAP_EXECUTION_LEARNINGS.md` ‚Äî This BL-020 entry
- `governance/execution/WAVE_MODEL.md` ‚Äî Wave authorization flow requires checklist

**Cross-Repository Impact**:
- FM repo: Create `governance/specs/FM_PREAUTH_CHECKLIST.md`
- FM repo: Update `.github/agents/ForemanApp-agent.md` with checklist binding
- All future wave/subwave authorizations blocked until checklist PASS

---

### Mandatory Requirements (Permanent, Platform-Wide)

All future FM planning and authorization MUST include:

1. **Checklist Execution Before Authorization**
   - FM MUST execute FM Pre-Authorization Checklist before ANY wave/subwave authorization
   - FM MUST execute checklist before ANY builder appointment
   - FM MUST execute checklist before re-authorizing blocked work

2. **Checklist Items Validated**
   - QA Catalog Alignment (per BL-018/BL-019)
   - QA-to-Red Foundation (tests exist and RED)
   - Architecture Alignment (architecture covers scope)
   - BL/FL-CI Ratchet Status (no blocking learnings)
   - Dependency Gates (all dependencies satisfied)

3. **Evidence Recording**
   - Checklist execution recorded in wave/subwave planning document
   - PASS/FAIL result with date and validation references
   - If FAIL: Documented blockers, corrections, re-execution

4. **Authorization Blocking**
   - Checklist FAIL blocks authorization absolutely
   - No builder appointments without checklist PASS
   - No "READY FOR AUTHORIZATION" status without checklist PASS

---

### Prohibited Actions (Permanent, Platform-Wide)

- ‚ùå Authorizing waves/subwaves without executing FM Pre-Authorization Checklist
- ‚ùå Issuing builder appointments without checklist PASS
- ‚ùå Proceeding with authorization when checklist items FAIL
- ‚ùå Treating checklist execution as optional or implicit
- ‚ùå Omitting checklist execution evidence from planning documents
- ‚ùå Skipping checklist re-execution after BL/FL-CI corrections

---

### Ratchet Statement

**This learning establishes that FM authorization without an explicit, structured pre-authorization checklist is a catastrophic structural failure requiring immediate governance-level fix.**

This is a **first-time failure** (CATASTROPHIC classification) representing a **missing structural defense mechanism** rather than execution error. The system has learned and canonized the FM Pre-Authorization Checklist as permanent prevention.

**Second occurrences trigger EMERGENCY classification and TARP activation** ‚Äî though this should be structurally prevented by checklist enforcement.

---

### Status

**Recorded & Canonized** ‚Äî Platform-Wide, Non-Retroactive  
**Applies To:** All FM instances in all repositories with wave-based delivery  
**Effective:** 2026-01-05

---

### Cross-References

**FM Office App (Source)**:
- Issue reference for Subwave 2.5 authorization failure
- `BOOTSTRAP_EXECUTION_LEARNINGS.md` ‚Äî BL-020 detailed entry (if exists)
- `FLCI_REGISTRY.md` ‚Äî BL-020 registry entry

**Governance Repo (Canonical)**:
- `governance/canon/FM_PREAUTH_CHECKLIST_CANON.md` ‚Äî NEW canonical checklist
- `governance/canon/ARCHITECTURE_COMPLETENESS_REQUIREMENTS.md` ¬ß 3.14 ‚Äî Checklist reference
- `governance/canon/LEARNING_INTAKE_AND_PROMOTION_MODEL.md` ¬ß 6.3 ‚Äî Forward-scan integration
- `governance/canon/BOOTSTRAP_EXECUTION_LEARNINGS.md` ‚Äî This BL-020 entry
- `governance/execution/WAVE_MODEL.md` ‚Äî Wave authorization flow update
- `governance/canon/GOVERNANCE_CANON_MANIFEST.md` ‚Äî Checklist added as PUBLIC_API

**Layer-Down Targets** (FM Repo):
- `governance/specs/FM_PREAUTH_CHECKLIST.md` ‚Äî FM-local checklist (Stage 2)
- `.github/agents/ForemanApp-agent.md` ‚Äî Agent contract binding (Stage 3)

---

**End of BL-020**

---

## BL-021 ‚Äî Incorrect Test Removal Due to Wrong Traceability Methodology (INCIDENT-2026-01-08)

### Classification

**CATASTROPHIC** ‚Äî First-Time Structural Failure (Missing Governance Gate + Wrong Methodology)

### Context

Wave 0 test suite review and technical debt assessment

### Incident Summary

60 Wave 0 RED tests were proposed for removal with justification that they were "speculative" and lacked architectural basis. Initial analysis used incorrect traceability methodology (searching for implementation class names in architecture rather than mapping test behaviors to architectural requirements).

Upon deeper review using correct methodology, **all 60 tests were found to be architecturally grounded**, validating explicit or implied requirements from the architecture specifications.

### Observed Issue

Tests validating architectural requirements were incorrectly classified as "ungrounded" due to:
1. Wrong traceability methodology (class name matching vs requirement mapping)
2. Abstraction level confusion (expecting architecture to specify implementation details)
3. Misunderstanding of test purpose (driving implementation vs validating existing code)
4. No governance gate for test removal decisions

### Root Cause Analysis

**Primary Cause**: No documented standard methodology for test-to-architecture traceability analysis

**Contributing Factors**:
1. **Methodology Gap**: No canonical definition of correct vs incorrect traceability approach
2. **Governance Gap**: No test removal governance gate requiring evidence and approval
3. **Training Gap**: Agents not trained on abstraction levels (architecture = requirements, tests = validation, implementation = classes/methods)
4. **Process Gap**: No requirement to map tests to architectural requirements before removal
5. **Misunderstanding of Test Role**: Treating unimplemented features as invalid test targets (tests DRIVE implementation, don't just validate existing code)

**Why This Matters**: Test removal based on flawed analysis:
- Loses required architectural QA coverage
- Removes specification traceability
- Eliminates regression protection
- Creates false impression that architecture is incomplete

### Specific Examples of Methodology Error

**Example 1: Evidence Schema Validation**
- ‚ùå Wrong: "No 'EvidenceSchemaValidator' class in architecture ‚Üí Test ungrounded"
- ‚úÖ Correct: "Architecture requires 'auditable evidence' ‚Üí Requires structure ‚Üí Requires validation ‚Üí Schema validation implied ‚Üí Test valid"

**Example 2: Silence Detection Heartbeat**
- ‚ùå Wrong: "Architecture says silence detection, not heartbeat ‚Üí Test ungrounded"
- ‚úÖ Correct: "Silence detection REQUIRES heartbeat/update signal ‚Üí Heartbeat functionally necessary ‚Üí Test valid"

**Example 3: Governance Enforcement Mechanisms**
- ‚ùå Wrong: "No 'ArchitectureFreezeEnforcer' class in architecture ‚Üí Test ungrounded"
- ‚úÖ Correct: "Architecture specifies 'Governance Supremacy Enforcer' ‚Üí Enforcement mechanisms implied ‚Üí Test valid"

### Learning

**Core Lesson**: Test-to-architecture traceability MUST use requirement-based mapping, not implementation-based matching.

**Correct Methodology**:
```
Test ‚Üí Validates What Behavior? ‚Üí Required By Which Requirement? ‚Üí Architecture Section?
```

**Incorrect Methodology**:
```
Test ‚Üí References What Class? ‚Üí Search Architecture for Class Name ‚Üí Not Found = Ungrounded
```

**Key Principles Established**:
1. Architecture specifies REQUIREMENTS (what), not IMPLEMENTATION (how)
2. Tests validate BEHAVIORS, not specific class/method existence
3. Implied requirements are still requirements (e.g., "auditable" implies "validated structure")
4. Tests for unimplemented features are VALID (tests drive implementation)
5. Component-functional requirements are valid (e.g., silence detection requires heartbeat)

### Governance Impact

**Immediate Actions Taken**:
1. Created **TEST_REMOVAL_GOVERNANCE_GATE.md** policy
   - Zero-tolerance policy for test removal
   - Required evidence: traceability analysis, impact assessment, alternative coverage
   - Approval requirements based on test count and risk
   - Prohibited justifications documented
   - Enforcement and violation response defined

2. Created **ARCHITECTURE_TEST_TRACEABILITY_METHODOLOGY.md** policy
   - Correct vs incorrect methodology documented
   - Abstraction level principles established
   - Decision tree for traceability
   - Common scenarios and training examples
   - Anti-patterns to avoid

3. Recorded this bootstrap learning (BL-021)

**Forward-Binding Expectations**:
- All test removal proposals MUST use correct traceability methodology
- All test removal proposals MUST follow governance gate process
- All agents MUST be trained on traceability methodology before authorization
- Repeat violations trigger agent contract review

### Prohibited Actions (Permanent, Platform-Wide)

- ‚ùå Removing tests without traceability analysis
- ‚ùå Using class name matching for traceability
- ‚ùå Assuming unimplemented features invalidate tests
- ‚ùå Expecting architecture to specify implementation details
- ‚ùå Removing tests due to being "noisy" or "slowing development"
- ‚ùå Bypassing test removal governance gate
- ‚ùå Insufficient evidence or impact analysis for test removal

### Ratchet Statement

**This learning establishes that test removal without proper traceability analysis and governance approval is a catastrophic governance failure requiring immediate restoration and incident response.**

This is a **first-time failure** (CATASTROPHIC classification) representing **missing governance infrastructure** (no test removal gate + no traceability methodology). The system has learned and canonized both policies as permanent prevention.

**Second occurrences trigger EMERGENCY classification** ‚Äî indicating either:
- Agent training failure
- Agent contract ambiguity
- Governance communication failure
- Deliberate bypass (most severe)

### Status

**Recorded & Canonized** ‚Äî Platform-Wide, Non-Retroactive  
**Applies To:** All repositories with architectural QA requirements  
**Effective:** 2026-01-08

### Impact Assessment

**Tests Affected**: 60 Wave 0 RED tests  
**Coverage Impact**: Architectural requirement validation across evidence, governance enforcement, escalation, and decision tracking components  
**Resolution**: Tests retained; traceability documented; governance policies established  
**Technical Debt Impact**: No debt created; proper governance prevents future occurrences

### Cross-References

**Governance Repo (Canonical)**:
- `governance/policy/TEST_REMOVAL_GOVERNANCE_GATE.md` ‚Äî Test removal policy (NEW)
- `governance/policy/ARCHITECTURE_TEST_TRACEABILITY_METHODOLOGY.md` ‚Äî Traceability methodology (NEW)
- `governance/canon/BOOTSTRAP_EXECUTION_LEARNINGS.md` ‚Äî This BL-021 entry
- `governance/policy/QA_POLICY_MASTER.md` ‚Äî QA coverage doctrine
- `governance/canon/REQUIREMENT_SPECIFICATION_GOVERNANCE.md` ‚Äî Requirements abstraction levels

**Layer-Down Targets** (All Application Repos):
- FM agent contracts ‚Äî Test removal governance gate reference
- Builder agent contracts ‚Äî Traceability methodology training requirement
- QA handover policies ‚Äî Test removal process integration

**Reference Materials**:
- INCIDENT-2026-01-08-INCORRECT-TEST-REMOVAL ‚Äî Incident report (if created)
- PR #470 analysis ‚Äî Original test removal proposal and review

### Prevention Measures

**Structural**:
1. Test Removal Governance Gate (mandatory approval process)
2. Traceability Methodology Canon (standardized analysis approach)
3. Agent training on abstraction levels
4. Evidence requirements for test removal
5. Approval thresholds based on risk

**Procedural**:
1. All test removal requires traceability analysis
2. All traceability analysis reviewed for methodology correctness
3. High-volume removals (>10 tests) require CS2 approval
4. All removals documented in TEST_REMOVAL_LOG.md
5. Bootstrap learning captured if systemic gap revealed

**Cultural**:
1. Tests drive implementation (not just validate existing code)
2. Unimplemented features are valid test targets
3. Architecture specifies requirements, not implementations
4. Implied requirements are still requirements
5. Burden of proof on remover, not reviewer

---

**End of BL-021**

---


